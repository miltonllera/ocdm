# @package _global_

defaults:
  - override /dataset: mpi3d
  - override /model: slot_ae
  - override /callbacks: default
  - override /trainer: gpu
  - _self_

# Tags format is 'training_regime', 'dataset', 'model', 'condition'
# This information is used by Hydra to create the run folder
# See configs/hydra/default.yaml
tags: ["unsupervised", "mpi3d", "slot_ae", "cylinder_to_vertical_axis"]

seed: null

test: True

model:
  slot:
    n_slots: 3
  # encoder:
  #   layer_defs:
  #     - [conv, [64, 4, 2, 1]]
  #     - [relu]
  #     - [conv, [64, 4, 2, 1]]
  #     - [relu]
  #     - [permute, [0, 2, 3, 1]]
  #     - [posemb2d, {embed: cardinal}]
  #     - [flatten, [1, 2]]
  #     - [layer_norm, [-1]]
  #     - [linear, ["${model.slot.input_size}"]]
  #     - [relu]
  # decoder:
  #   layer_defs:
  #     - [spatbroad, [16, 16], {input_last: True}]
  #     - [posemb2d, {embed: cardinal}]
  #     - [permute, [0, 3, 1, 2]]
  #     - [tconv, [64, 4, 2, 1]]
  #     - [relu]
  #     - [tconv, ["${sum:${model.encoder.input_size.0},1}", 4, 2, 1]]

  training:
    optimizer:
      lr: 0.0004

    schedulers:
      exponential:
        gamma: 0.999993068552217  # 0.5 ** (1 / 100000)

dataset:
  batch_size: 64
  split_sizes: [0.95, 0.05]
  split_condition: combgen
  split_variant: cyl2vx
  split_modifiers: [four_shapes, lhalf_hx]

trainer:
  min_epochs: 1
  max_epochs: 50
  gradient_clip_val: 1.0

callbacks:
  model_checkpoint:
    mode: "min"
    monitor: "val/loss"
